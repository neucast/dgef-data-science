{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "class MonteCarloAgent():\n",
    "    def __init__(self, possibleActions,discountFactor, epsilon, initVals=0.0):\n",
    "        self.possibleActions = possibleActions\n",
    "        self.discountFactor = discountFactor\n",
    "        self.epsilon = epsilon\n",
    "        self.state = []\n",
    "        self.episode = []\n",
    "        self.G = initVals\n",
    "        self.QValues = defaultdict(lambda : {})\n",
    "        self.policy = defaultdict(float)\n",
    "        self.returnsSum = defaultdict(float)\n",
    "        self.returnsCount = defaultdict(float)\n",
    "        self.numPossActions = len(self.possibleActions)\n",
    "        \n",
    "\n",
    "    def getPolicy(self):\n",
    "\n",
    "        # Generate e-greedy policy\n",
    "        if self.state in self.QValues.keys():\n",
    "            for action in self.possibleActions:\n",
    "                self.policy[action] = self.epsilon / self.numPossActions\n",
    "            bestAction = max(self.QValues[self.state].items(), key=operator.itemgetter(1))[0] \n",
    "            self.policy[bestAction] += (1.0 - self.epsilon)  \n",
    "        else:\n",
    "            for action in self.possibleActions:    \n",
    "                self.policy[action] = 1 / self.numPossActions\n",
    "                \n",
    "        #print(\"Policy to take action : \")\n",
    "        #for k, v in self.policy.items():\n",
    "         #   print(\"action \",k,\"Choose prob \",v)\n",
    "\n",
    "\n",
    "    def toStateRepresentation(self, state):\n",
    "        return tuple((state[0],state[1]))\n",
    "    \n",
    "\n",
    "    def setExperience(self, state, action, reward):\n",
    "        #Generates an episode with all states visited \n",
    "        self.episode.append((state,action,reward))\n",
    "\n",
    "\n",
    "    def setState(self, state):\n",
    "        self.state = state    \n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = []\n",
    "        self.episode = []\n",
    "\n",
    "    def act(self):\n",
    "        # Take an action\n",
    "        self.getPolicy()\n",
    "        probs = list(self.policy.values())\n",
    "        actions = list(self.policy.keys())\n",
    "        action = actions[np.random.choice(np.arange(len(probs)), p=probs)]\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def setEpsilon(self, epsilon):\n",
    "        self.epsilon = epsilon \n",
    "\n",
    "    def computeHyperparameters(self):\n",
    "        return self.epsilon\n",
    "\n",
    "    \n",
    "    def learn(self):  \n",
    "        \n",
    "      #  print(\"Episode: \",self.episode)\n",
    "        sa_in_episode = set([ tuple((x[0], x[1])) for x in self.episode])\n",
    "            \n",
    "\n",
    "        QEstimate = []\n",
    "        QEstimateDict = {}\n",
    "        \n",
    "        # Loop for each pair of state-action in an episode\n",
    "        for state, action in sa_in_episode:\n",
    "\n",
    "            sa_pair = (state, action) \n",
    "       #     print(\"State-action pair: \",sa_pair)\n",
    "            \n",
    "            # Get the index of the first ocurrence of a pair\n",
    "            first_occurence_idx = next(i for i,x in enumerate(self.episode) if x[0] == state and x[1] == action)\n",
    "            \n",
    "            # Sum the rewards since the first visit to state-action \n",
    "            G = sum([x[2]*(self.discountFactor**i) for i,x in enumerate(self.episode[first_occurence_idx:])])\n",
    "            self.returnsSum[sa_pair] += G\n",
    "            self.returnsCount[sa_pair] += 1.0\n",
    "            \n",
    "\n",
    "         #   print(\"Numero de visitas : \")\n",
    "          #  for k, v in self.returnsCount.items():\n",
    "           #     print(k,v)\n",
    "\n",
    "            \n",
    "            # Set the Q values of every state not visited before to zero\n",
    "            if not state in self.QValues.keys():\n",
    "                for action in self.possibleActions:\n",
    "                    self.QValues[state][action] = 0\n",
    "\n",
    "            self.QValues[state][action] = self.returnsSum[sa_pair] / self.returnsCount[sa_pair]   \n",
    "            \n",
    "            \n",
    "       # print(\"Funci√≥n de valor Q: \")\n",
    "       # for k, v in self.QValues.items():\n",
    "        #    print(k,v)\n",
    "            \n",
    "        return self.QValues\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible actions:  ['0', '1']\n",
      "\n",
      "******************************* EPISODE  0 ***************************\n",
      "State after reset:  (18, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (25, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  1 ***************************\n",
      "State after reset:  (12, 7, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (12, 7, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  2 ***************************\n",
      "State after reset:  (17, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (27, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  3 ***************************\n",
      "State after reset:  (17, 2, True)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (17, 2, True)\n",
      "Next reward:  1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  4 ***************************\n",
      "State after reset:  (18, 7, True)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (14, 7, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (20, 7, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (26, 7, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  5 ***************************\n",
      "State after reset:  (13, 2, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (20, 2, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (28, 2, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  6 ***************************\n",
      "State after reset:  (13, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (13, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  7 ***************************\n",
      "State after reset:  (20, 10, True)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (20, 10, True)\n",
      "Next reward:  1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  8 ***************************\n",
      "State after reset:  (18, 1, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (18, 1, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  9 ***************************\n",
      "State after reset:  (20, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.45\n",
      "action  1 Choose prob  0.55\n",
      "Action:  1\n",
      "Next state:  (21, 10, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (22, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  10 ***************************\n",
      "State after reset:  (9, 2, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (9, 2, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  11 ***************************\n",
      "State after reset:  (13, 10, True)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.55\n",
      "action  1 Choose prob  0.45\n",
      "Action:  0\n",
      "Next state:  (13, 10, True)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  12 ***************************\n",
      "State after reset:  (21, 5, True)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (12, 5, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (12, 5, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  13 ***************************\n",
      "State after reset:  (7, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (18, 10, True)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.55\n",
      "action  1 Choose prob  0.45\n",
      "Action:  1\n",
      "Next state:  (18, 10, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.55\n",
      "action  1 Choose prob  0.45\n",
      "Action:  1\n",
      "Next state:  (28, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  14 ***************************\n",
      "State after reset:  (11, 3, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (11, 3, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  15 ***************************\n",
      "State after reset:  (16, 2, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (16, 2, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  16 ***************************\n",
      "State after reset:  (5, 8, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (5, 8, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  17 ***************************\n",
      "State after reset:  (14, 7, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.55\n",
      "action  1 Choose prob  0.45\n",
      "Action:  0\n",
      "Next state:  (14, 7, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  18 ***************************\n",
      "State after reset:  (21, 9, True)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (21, 9, True)\n",
      "Next reward:  1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  19 ***************************\n",
      "State after reset:  (15, 8, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (15, 8, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  20 ***************************\n",
      "State after reset:  (19, 3, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (25, 3, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  21 ***************************\n",
      "State after reset:  (11, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (11, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  22 ***************************\n",
      "State after reset:  (16, 10, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (16, 10, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  23 ***************************\n",
      "State after reset:  (13, 8, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (13, 8, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  24 ***************************\n",
      "State after reset:  (11, 1, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (21, 1, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (21, 1, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  25 ***************************\n",
      "State after reset:  (13, 4, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (22, 4, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  26 ***************************\n",
      "State after reset:  (15, 1, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (19, 1, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (29, 1, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  27 ***************************\n",
      "State after reset:  (15, 5, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (15, 5, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  28 ***************************\n",
      "State after reset:  (4, 1, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  1\n",
      "Next state:  (9, 1, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.5\n",
      "action  1 Choose prob  0.5\n",
      "Action:  0\n",
      "Next state:  (9, 1, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n",
      "\n",
      "******************************* EPISODE  29 ***************************\n",
      "State after reset:  (9, 1, False)\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.55\n",
      "action  1 Choose prob  0.45\n",
      "Action:  1\n",
      "Next state:  (19, 1, False)\n",
      "Next reward:  0.0\n",
      "Episode finished:  False\n",
      "Policy to take action : \n",
      "action  0 Choose prob  0.55\n",
      "action  1 Choose prob  0.45\n",
      "Action:  1\n",
      "Next state:  (24, 1, False)\n",
      "Next reward:  -1.0\n",
      "Episode finished:  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    env = gym.make('Blackjack-v1')\n",
    "    space_size = env.action_space.n\n",
    "    possibleActions = []\n",
    "    for i in range(space_size):\n",
    "        possibleActions.append(str(i))\n",
    "        \n",
    "    print(\"Possible actions: \",possibleActions)\n",
    "\n",
    "\n",
    "    # Initialize a Monte-Carlo Agent\n",
    "    agent = MonteCarloAgent(possibleActions,discountFactor = 0.999, epsilon = 0.9)\n",
    "\n",
    "    # Run training Monte Carlo Method\n",
    "    for episode in range(30):\n",
    "        print(\"\\n******************************* EPISODE \",episode,\"***************************\")\n",
    "        agent.reset()\n",
    "        observation = env.reset()   # Returns current state\n",
    "        print(\"State after reset: \",observation)\n",
    "       \n",
    "        done = False\n",
    "    \n",
    "        while done==False:\n",
    "            \n",
    "            epsilon = agent.computeHyperparameters()\n",
    "            agent.setEpsilon(epsilon)\n",
    "            obsCopy = observation   # Copy current state\n",
    "            agent.setState(agent.toStateRepresentation(obsCopy))\n",
    "            action = agent.act()\n",
    "\n",
    "            print(\"Action: \",action)\n",
    "            \n",
    "            nextObservation, reward, done, status = env.step(int(action))\n",
    "\n",
    "            \n",
    "            print(\"Next state: \",nextObservation)\n",
    "            print(\"Next reward: \",reward)\n",
    "            \n",
    "            print(\"Episode finished: \",done)\n",
    "            agent.setExperience(agent.toStateRepresentation(obsCopy), str(action), reward)\n",
    "            observation = nextObservation\n",
    "            \n",
    "\n",
    "        QValues = agent.learn()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q value function: \n",
      "item  1 (4, 1) {'0': -0.8, '1': -0.99858383325}\n",
      "item  2 (4, 2) {'0': -0.14285714285714285, '1': -0.5707144285714285}\n",
      "item  3 (4, 3) {'0': -0.75, '1': -0.1998999}\n",
      "item  4 (4, 4) {'0': -0.75, '1': -0.33286679999999996}\n",
      "item  5 (4, 5) {'0': 0.0, '1': -0.14278564285714285}\n",
      "item  6 (4, 6) {'0': 0.38461538461538464, '1': -0.3120625625624999}\n",
      "item  7 (4, 7) {'0': -0.6, '1': -0.17647035299999997}\n",
      "item  8 (4, 8) {'0': -0.25, '1': -0.42771492828578567}\n",
      "item  9 (4, 9) {'0': -0.4444444444444444, '1': -0.6352731818181819}\n",
      "item  10 (4, 10) {'0': -0.5609756097560976, '1': -0.7547325850001215}\n",
      "item  11 (5, 1) {'0': -0.9047619047619048, '1': -0.6833160525789475}\n",
      "item  12 (5, 2) {'0': -0.3333333333333333, '1': -0.39945034975005}\n",
      "item  13 (5, 3) {'0': -0.21428571428571427, '1': -0.3041302175217392}\n",
      "item  14 (5, 4) {'0': 0.03225806451612903, '1': -0.124916625}\n",
      "item  15 (5, 5) {'0': 0.08333333333333333, '1': -0.30408704330439124}\n",
      "item  16 (5, 6) {'0': -0.09090909090909091, '1': 0.07135714285714286}\n",
      "item  17 (5, 7) {'0': -0.6521739130434783, '1': -0.47547647614285704}\n",
      "item  18 (5, 8) {'0': -0.3333333333333333, '1': 0.0001664167500000091}\n",
      "item  19 (5, 9) {'0': -0.5333333333333333, '1': -0.7989502999500001}\n",
      "item  20 (5, 10) {'0': -0.5268817204301075, '1': -0.5722475279101801}\n",
      "item  21 (6, 1) {'0': -0.6, '1': -0.6355683636136361}\n",
      "item  22 (6, 2) {'0': -0.07692307692307693, '1': -0.4495250250000001}\n",
      "item  23 (6, 3) {'0': -0.14285714285714285, '1': -0.18161361368181822}\n",
      "item  24 (6, 4) {'0': -0.3333333333333333, '1': -0.3595201200000001}\n",
      "item  25 (6, 5) {'0': -0.02857142857142857, '1': -0.4383905364634391}\n",
      "item  26 (6, 6) {'0': 0.0, '1': -0.3221935806774194}\n",
      "item  27 (6, 7) {'0': -0.4418604651162791, '1': -0.5287648235294117}\n",
      "item  28 (6, 8) {'0': -0.5428571428571428, '1': -0.665714642785714}\n",
      "item  29 (6, 9) {'0': -0.41935483870967744, '1': -0.6443550322258064}\n",
      "item  30 (6, 10) {'0': -0.6, '1': -0.6419764761349288}\n",
      "item  31 (7, 1) {'0': -0.7368421052631579, '1': -0.7008773859473684}\n",
      "item  32 (7, 2) {'0': -0.4, '1': -0.4674682340425531}\n",
      "item  33 (7, 3) {'0': -0.5, '1': -0.15233901696610172}\n",
      "item  34 (7, 4) {'0': -0.2682926829268293, '1': -0.60379097672093}\n",
      "item  35 (7, 5) {'0': -0.43859649122807015, '1': -0.17535073689473687}\n",
      "item  36 (7, 6) {'0': -0.28, '1': -0.13615918179545453}\n",
      "item  37 (7, 7) {'0': -0.47619047619047616, '1': -0.5312342127446806}\n",
      "item  38 (7, 8) {'0': -0.627906976744186, '1': -0.4623150739814999}\n",
      "item  39 (7, 9) {'0': -0.5909090909090909, '1': -0.5449091136136363}\n",
      "item  40 (7, 10) {'0': -0.6702127659574468, '1': -0.6379836665888997}\n",
      "item  41 (8, 1) {'0': -0.9230769230769231, '1': -0.8234564736140529}\n",
      "item  42 (8, 2) {'0': -0.125, '1': -0.3210536071607143}\n",
      "item  43 (8, 3) {'0': -0.4444444444444444, '1': -0.31707944442857144}\n",
      "item  44 (8, 4) {'0': -0.07692307692307693, '1': -0.21290162296721313}\n",
      "item  45 (8, 5) {'0': -0.2222222222222222, '1': -0.1997001333}\n",
      "item  46 (8, 6) {'0': -0.22807017543859648, '1': -0.07401851848149998}\n",
      "item  47 (8, 7) {'0': -0.5428571428571428, '1': -0.3514445}\n",
      "item  48 (8, 8) {'0': -0.34328358208955223, '1': -0.34496385447272726}\n",
      "item  49 (8, 9) {'0': -0.5, '1': -0.6380558332361252}\n",
      "item  50 (8, 10) {'0': -0.6030534351145038, '1': -0.5132727786482246}\n",
      "item  51 (9, 1) {'0': -0.7101449275362319, '1': -0.7288785135000004}\n",
      "item  52 (9, 2) {'0': -0.4157303370786517, '1': -0.2577742419193549}\n",
      "item  53 (9, 3) {'0': -0.3409090909090909, '1': -0.2654937215316455}\n",
      "item  54 (9, 4) {'0': -0.38181818181818183, '1': -0.38527149998571414}\n",
      "item  55 (9, 5) {'0': -0.18181818181818182, '1': -0.36002334882558124}\n",
      "item  56 (9, 6) {'0': -0.2830188679245283, '1': -0.09196060523684207}\n",
      "item  57 (9, 7) {'0': -0.3783783783783784, '1': -0.11894048807142858}\n",
      "item  58 (9, 8) {'0': -0.5263157894736842, '1': -0.17558251645054948}\n",
      "item  59 (9, 9) {'0': -0.7101449275362319, '1': -0.34846510468604636}\n",
      "item  60 (9, 10) {'0': -0.5639097744360902, '1': -0.5606703824140411}\n",
      "item  61 (10, 1) {'0': -0.7333333333333333, '1': -0.41208749999999983}\n",
      "item  62 (10, 2) {'0': -0.22666666666666666, '1': -0.34694746313684194}\n",
      "item  63 (10, 3) {'0': -0.34210526315789475, '1': -0.47616291858139537}\n",
      "item  64 (10, 4) {'0': -0.24444444444444444, '1': -0.12333333333333334}\n",
      "item  65 (10, 5) {'0': -0.012345679012345678, '1': -0.23633331183870962}\n",
      "item  66 (10, 6) {'0': -0.1262135922330097, '1': -0.1134773295340909}\n",
      "item  67 (10, 7) {'0': -0.582089552238806, '1': -0.13082138101189286}\n",
      "item  68 (10, 8) {'0': -0.4639175257731959, '1': -0.1712571285857143}\n",
      "item  69 (10, 9) {'0': -0.45652173913043476, '1': -0.20846167031868132}\n",
      "item  70 (10, 10) {'0': -0.6060606060606061, '1': -0.46079842539502935}\n",
      "item  71 (11, 1) {'0': -0.8217821782178217, '1': -0.5266728000000004}\n",
      "item  72 (11, 2) {'0': -0.16, '1': -0.4848254077281556}\n",
      "item  73 (11, 3) {'0': -0.2033898305084746, '1': -0.36703056122448974}\n",
      "item  74 (11, 4) {'0': -0.09278350515463918, '1': -0.19564951546391754}\n",
      "item  75 (11, 5) {'0': -0.2087912087912088, '1': -0.3835535535803572}\n",
      "item  76 (11, 6) {'0': -0.11538461538461539, '1': -0.13665269472631578}\n",
      "item  77 (11, 7) {'0': -0.43636363636363634, '1': -0.26138321495327094}\n",
      "item  78 (11, 8) {'0': -0.37037037037037035, '1': -0.28680202969306917}\n",
      "item  79 (11, 9) {'0': -0.6483516483516484, '1': -0.419196455357143}\n",
      "item  80 (11, 10) {'0': -0.5139664804469274, '1': -0.38217867091071367}\n",
      "item  81 (12, 1) {'0': -0.7821782178217822, '1': -0.6985232546157399}\n",
      "item  82 (12, 2) {'0': -0.30526315789473685, '1': -0.5291719502307689}\n",
      "item  83 (12, 3) {'0': -0.3901345291479821, '1': -0.6062228720000042}\n",
      "item  84 (12, 4) {'0': -0.3059360730593607, '1': -0.4782774976124399}\n",
      "item  85 (12, 5) {'0': -0.10599078341013825, '1': -0.47769025663716785}\n",
      "item  86 (12, 6) {'0': -0.1659919028340081, '1': -0.39710505478995417}\n",
      "item  87 (12, 7) {'0': -0.4956521739130435, '1': -0.51176073207655}\n",
      "item  88 (12, 8) {'0': -0.4666666666666667, '1': -0.4597112299518716}\n",
      "item  89 (12, 9) {'0': -0.5979899497487438, '1': -0.5492117927837835}\n",
      "item  90 (12, 10) {'0': -0.5973741794310722, '1': -0.6116316009889843}\n",
      "item  91 (13, 1) {'0': -0.7283950617283951, '1': -0.8255556569710181}\n",
      "item  92 (13, 2) {'0': -0.3452914798206278, '1': -0.6123378468603554}\n",
      "item  93 (13, 3) {'0': -0.13170731707317074, '1': -0.4742837441767442}\n",
      "item  94 (13, 4) {'0': -0.19469026548672566, '1': -0.5256288350721595}\n",
      "item  95 (13, 5) {'0': -0.2653061224489796, '1': -0.49761087329864256}\n",
      "item  96 (13, 6) {'0': -0.17703349282296652, '1': -0.5210234178544598}\n",
      "item  97 (13, 7) {'0': -0.3064516129032258, '1': -0.41651756140350865}\n",
      "item  98 (13, 8) {'0': -0.5495495495495496, '1': -0.5528672787654866}\n",
      "item  99 (13, 9) {'0': -0.5132743362831859, '1': -0.6095919551524659}\n",
      "item  100 (13, 10) {'0': -0.578838174273859, '1': -0.675423184722299}\n",
      "item  101 (14, 1) {'0': -0.7280701754385965, '1': -0.7713933058058245}\n",
      "item  102 (14, 2) {'0': -0.2742616033755274, '1': -0.5502773235252097}\n",
      "item  103 (14, 3) {'0': -0.1717171717171717, '1': -0.5652025569620251}\n",
      "item  104 (14, 4) {'0': -0.18627450980392157, '1': -0.5523561963378993}\n",
      "item  105 (14, 5) {'0': -0.11790393013100436, '1': -0.5608869547420812}\n",
      "item  106 (14, 6) {'0': -0.1013215859030837, '1': -0.56295636892233}\n",
      "item  107 (14, 7) {'0': -0.49238578680203043, '1': -0.48514351982177706}\n",
      "item  108 (14, 8) {'0': -0.5428571428571428, '1': -0.5504449603480175}\n",
      "item  109 (14, 9) {'0': -0.43478260869565216, '1': -0.6028088186323528}\n",
      "item  110 (14, 10) {'0': -0.6125290023201856, '1': -0.715221010558171}\n",
      "item  111 (15, 1) {'0': -0.7767857142857143, '1': -0.7666017796483044}\n",
      "item  112 (15, 2) {'0': -0.17012448132780084, '1': -0.5910266666711109}\n",
      "item  113 (15, 3) {'0': -0.21973094170403587, '1': -0.6487819255372339}\n",
      "item  114 (15, 4) {'0': -0.1, '1': -0.624295774647887}\n",
      "item  115 (15, 5) {'0': -0.1896551724137931, '1': -0.6390411415479451}\n",
      "item  116 (15, 6) {'0': -0.10309278350515463, '1': -0.5111479999999999}\n",
      "item  117 (15, 7) {'0': -0.5660377358490566, '1': -0.560328492748792}\n",
      "item  118 (15, 8) {'0': -0.45918367346938777, '1': -0.6715252777727271}\n",
      "item  119 (15, 9) {'0': -0.6521739130434783, '1': -0.6026168551214997}\n",
      "item  120 (15, 10) {'0': -0.5985832349468713, '1': -0.6988056569110079}\n",
      "item  121 (16, 1) {'0': -0.75, '1': -0.7638146516797748}\n",
      "item  122 (16, 2) {'0': -0.42105263157894735, '1': -0.5202908469336736}\n",
      "item  123 (16, 3) {'0': -0.2571428571428571, '1': -0.6637185252058821}\n",
      "item  124 (16, 4) {'0': -0.23478260869565218, '1': -0.560644878495327}\n",
      "item  125 (16, 5) {'0': -0.1324200913242009, '1': -0.6249305509166713}\n",
      "item  126 (16, 6) {'0': -0.16908212560386474, '1': -0.4999948402164947}\n",
      "item  127 (16, 7) {'0': -0.5648148148148148, '1': -0.6189619095333335}\n",
      "item  128 (16, 8) {'0': -0.5213675213675214, '1': -0.6322059313725489}\n",
      "item  129 (16, 9) {'0': -0.5844155844155844, '1': -0.6920769683212665}\n",
      "item  130 (16, 10) {'0': -0.5767918088737202, '1': -0.7207766183514515}\n",
      "item  131 (17, 1) {'0': -0.639269406392694, '1': -0.8359867610619461}\n",
      "item  132 (17, 2) {'0': -0.12017167381974249, '1': -0.6467018577935778}\n",
      "item  133 (17, 3) {'0': -0.08444444444444445, '1': -0.6798933644399997}\n",
      "item  134 (17, 4) {'0': -0.10362694300518134, '1': -0.7372458379832402}\n",
      "item  135 (17, 5) {'0': -0.0995260663507109, '1': -0.7135477468879664}\n",
      "item  136 (17, 6) {'0': 0.10434782608695652, '1': -0.6682368999894734}\n",
      "item  137 (17, 7) {'0': -0.03804347826086957, '1': -0.5938730862944162}\n",
      "item  138 (17, 8) {'0': -0.37264150943396224, '1': -0.6144469162178714}\n",
      "item  139 (17, 9) {'0': -0.41237113402061853, '1': -0.6619154694882626}\n",
      "item  140 (17, 10) {'0': -0.4632438739789965, '1': -0.7585525424950014}\n",
      "item  141 (18, 1) {'0': -0.3632075471698113, '1': -0.8487552135416663}\n",
      "item  142 (18, 2) {'0': 0.23115577889447236, '1': -0.7966577860962564}\n",
      "item  143 (18, 3) {'0': 0.08530805687203792, '1': -0.6750457106548223}\n",
      "item  144 (18, 4) {'0': 0.12195121951219512, '1': -0.7248148095291}\n",
      "item  145 (18, 5) {'0': 0.3333333333333333, '1': -0.6846630869510869}\n",
      "item  146 (18, 6) {'0': 0.3440366972477064, '1': -0.6718385572864581}\n",
      "item  147 (18, 7) {'0': 0.45739910313901344, '1': -0.6948135932146892}\n",
      "item  148 (18, 8) {'0': 0.03418803418803419, '1': -0.6192893400964468}\n",
      "item  149 (18, 9) {'0': -0.13471502590673576, '1': -0.8102891374218055}\n",
      "item  150 (18, 10) {'0': -0.3105446118192352, '1': -0.8031273959596485}\n",
      "item  151 (19, 1) {'0': -0.0625, '1': -0.8708156912442392}\n",
      "item  152 (19, 2) {'0': 0.3177570093457944, '1': -0.8018160377358491}\n",
      "item  153 (19, 3) {'0': 0.3791469194312796, '1': -0.7938794120603012}\n",
      "item  154 (19, 4) {'0': 0.43157894736842106, '1': -0.7470294176470589}\n",
      "item  155 (19, 5) {'0': 0.42790697674418604, '1': -0.75996002499}\n",
      "item  156 (19, 6) {'0': 0.5679611650485437, '1': -0.7905601780157067}\n",
      "item  157 (19, 7) {'0': 0.6609442060085837, '1': -0.7725118151753553}\n",
      "item  158 (19, 8) {'0': 0.5972222222222222, '1': -0.703306210535885}\n",
      "item  159 (19, 9) {'0': 0.2184873949579832, '1': -0.674767024262136}\n",
      "item  160 (19, 10) {'0': -0.03970223325062035, '1': -0.79767677028886}\n",
      "item  161 (20, 1) {'0': 0.17253521126760563, '1': -0.886536092783505}\n",
      "item  162 (20, 2) {'0': 0.6190476190476191, '1': -0.8445088515901061}\n",
      "item  163 (20, 3) {'0': 0.7044673539518901, '1': -0.8398469822064055}\n",
      "item  164 (20, 4) {'0': 0.6594427244582043, '1': -0.8800924657534246}\n",
      "item  165 (20, 5) {'0': 0.6241379310344828, '1': -0.7324154964788734}\n",
      "item  166 (20, 6) {'0': 0.7198581560283688, '1': -0.8826724517172411}\n",
      "item  167 (20, 7) {'0': 0.796875, '1': -0.7846970766423358}\n",
      "item  168 (20, 8) {'0': 0.8369230769230769, '1': -0.8208805940298505}\n",
      "item  169 (20, 9) {'0': 0.75625, '1': -0.8530979265629369}\n",
      "item  170 (20, 10) {'0': 0.4103194103194103, '1': -0.8902487253842841}\n",
      "item  171 (21, 1) {'0': 0.6305418719211823, '1': -0.7849843246125648}\n",
      "item  172 (21, 2) {'0': 0.9521276595744681, '1': -0.7172316892598867}\n",
      "item  173 (21, 3) {'0': 0.9306930693069307, '1': -0.5997177470411762}\n",
      "item  174 (21, 4) {'0': 0.945273631840796, '1': -0.5235392565497381}\n",
      "item  175 (21, 5) {'0': 0.9481132075471698, '1': -0.6152410102564099}\n",
      "item  176 (21, 6) {'0': 0.9471153846153846, '1': -0.6283657257142855}\n",
      "item  177 (21, 7) {'0': 0.9751243781094527, '1': -0.5323905502958577}\n",
      "item  178 (21, 8) {'0': 0.9801980198019802, '1': -0.6453015873068778}\n",
      "item  179 (21, 9) {'0': 0.9553072625698324, '1': -0.6895615080267374}\n",
      "item  180 (21, 10) {'0': 0.9143576826196473, '1': -0.6218866424418616}\n",
      "state:  (4, 1)  action:  0  value:  -0.8\n",
      "state:  (4, 2)  action:  0  value:  -0.14285714285714285\n",
      "state:  (4, 3)  action:  1  value:  -0.1998999\n",
      "state:  (4, 4)  action:  1  value:  -0.33286679999999996\n",
      "state:  (4, 5)  action:  0  value:  0.0\n",
      "state:  (4, 6)  action:  0  value:  0.38461538461538464\n",
      "state:  (4, 7)  action:  1  value:  -0.17647035299999997\n",
      "state:  (4, 8)  action:  0  value:  -0.25\n",
      "state:  (4, 9)  action:  0  value:  -0.4444444444444444\n",
      "state:  (4, 10)  action:  0  value:  -0.5609756097560976\n",
      "state:  (5, 1)  action:  1  value:  -0.6833160525789475\n",
      "state:  (5, 2)  action:  0  value:  -0.3333333333333333\n",
      "state:  (5, 3)  action:  0  value:  -0.21428571428571427\n",
      "state:  (5, 4)  action:  0  value:  0.03225806451612903\n",
      "state:  (5, 5)  action:  0  value:  0.08333333333333333\n",
      "state:  (5, 6)  action:  1  value:  0.07135714285714286\n",
      "state:  (5, 7)  action:  1  value:  -0.47547647614285704\n",
      "state:  (5, 8)  action:  1  value:  0.0001664167500000091\n",
      "state:  (5, 9)  action:  0  value:  -0.5333333333333333\n",
      "state:  (5, 10)  action:  0  value:  -0.5268817204301075\n",
      "state:  (6, 1)  action:  0  value:  -0.6\n",
      "state:  (6, 2)  action:  0  value:  -0.07692307692307693\n",
      "state:  (6, 3)  action:  0  value:  -0.14285714285714285\n",
      "state:  (6, 4)  action:  0  value:  -0.3333333333333333\n",
      "state:  (6, 5)  action:  0  value:  -0.02857142857142857\n",
      "state:  (6, 6)  action:  0  value:  0.0\n",
      "state:  (6, 7)  action:  0  value:  -0.4418604651162791\n",
      "state:  (6, 8)  action:  0  value:  -0.5428571428571428\n",
      "state:  (6, 9)  action:  0  value:  -0.41935483870967744\n",
      "state:  (6, 10)  action:  0  value:  -0.6\n",
      "state:  (7, 1)  action:  1  value:  -0.7008773859473684\n",
      "state:  (7, 2)  action:  0  value:  -0.4\n",
      "state:  (7, 3)  action:  1  value:  -0.15233901696610172\n",
      "state:  (7, 4)  action:  0  value:  -0.2682926829268293\n",
      "state:  (7, 5)  action:  1  value:  -0.17535073689473687\n",
      "state:  (7, 6)  action:  1  value:  -0.13615918179545453\n",
      "state:  (7, 7)  action:  0  value:  -0.47619047619047616\n",
      "state:  (7, 8)  action:  1  value:  -0.4623150739814999\n",
      "state:  (7, 9)  action:  1  value:  -0.5449091136136363\n",
      "state:  (7, 10)  action:  1  value:  -0.6379836665888997\n",
      "state:  (8, 1)  action:  1  value:  -0.8234564736140529\n",
      "state:  (8, 2)  action:  0  value:  -0.125\n",
      "state:  (8, 3)  action:  1  value:  -0.31707944442857144\n",
      "state:  (8, 4)  action:  0  value:  -0.07692307692307693\n",
      "state:  (8, 5)  action:  1  value:  -0.1997001333\n",
      "state:  (8, 6)  action:  1  value:  -0.07401851848149998\n",
      "state:  (8, 7)  action:  1  value:  -0.3514445\n",
      "state:  (8, 8)  action:  0  value:  -0.34328358208955223\n",
      "state:  (8, 9)  action:  0  value:  -0.5\n",
      "state:  (8, 10)  action:  1  value:  -0.5132727786482246\n",
      "state:  (9, 1)  action:  0  value:  -0.7101449275362319\n",
      "state:  (9, 2)  action:  1  value:  -0.2577742419193549\n",
      "state:  (9, 3)  action:  1  value:  -0.2654937215316455\n",
      "state:  (9, 4)  action:  0  value:  -0.38181818181818183\n",
      "state:  (9, 5)  action:  0  value:  -0.18181818181818182\n",
      "state:  (9, 6)  action:  1  value:  -0.09196060523684207\n",
      "state:  (9, 7)  action:  1  value:  -0.11894048807142858\n",
      "state:  (9, 8)  action:  1  value:  -0.17558251645054948\n",
      "state:  (9, 9)  action:  1  value:  -0.34846510468604636\n",
      "state:  (9, 10)  action:  1  value:  -0.5606703824140411\n",
      "state:  (10, 1)  action:  1  value:  -0.41208749999999983\n",
      "state:  (10, 2)  action:  0  value:  -0.22666666666666666\n",
      "state:  (10, 3)  action:  0  value:  -0.34210526315789475\n",
      "state:  (10, 4)  action:  1  value:  -0.12333333333333334\n",
      "state:  (10, 5)  action:  0  value:  -0.012345679012345678\n",
      "state:  (10, 6)  action:  1  value:  -0.1134773295340909\n",
      "state:  (10, 7)  action:  1  value:  -0.13082138101189286\n",
      "state:  (10, 8)  action:  1  value:  -0.1712571285857143\n",
      "state:  (10, 9)  action:  1  value:  -0.20846167031868132\n",
      "state:  (10, 10)  action:  1  value:  -0.46079842539502935\n",
      "state:  (11, 1)  action:  1  value:  -0.5266728000000004\n",
      "state:  (11, 2)  action:  0  value:  -0.16\n",
      "state:  (11, 3)  action:  0  value:  -0.2033898305084746\n",
      "state:  (11, 4)  action:  0  value:  -0.09278350515463918\n",
      "state:  (11, 5)  action:  0  value:  -0.2087912087912088\n",
      "state:  (11, 6)  action:  0  value:  -0.11538461538461539\n",
      "state:  (11, 7)  action:  1  value:  -0.26138321495327094\n",
      "state:  (11, 8)  action:  1  value:  -0.28680202969306917\n",
      "state:  (11, 9)  action:  1  value:  -0.419196455357143\n",
      "state:  (11, 10)  action:  1  value:  -0.38217867091071367\n",
      "state:  (12, 1)  action:  1  value:  -0.6985232546157399\n",
      "state:  (12, 2)  action:  0  value:  -0.30526315789473685\n",
      "state:  (12, 3)  action:  0  value:  -0.3901345291479821\n",
      "state:  (12, 4)  action:  0  value:  -0.3059360730593607\n",
      "state:  (12, 5)  action:  0  value:  -0.10599078341013825\n",
      "state:  (12, 6)  action:  0  value:  -0.1659919028340081\n",
      "state:  (12, 7)  action:  0  value:  -0.4956521739130435\n",
      "state:  (12, 8)  action:  1  value:  -0.4597112299518716\n",
      "state:  (12, 9)  action:  1  value:  -0.5492117927837835\n",
      "state:  (12, 10)  action:  0  value:  -0.5973741794310722\n",
      "state:  (13, 1)  action:  0  value:  -0.7283950617283951\n",
      "state:  (13, 2)  action:  0  value:  -0.3452914798206278\n",
      "state:  (13, 3)  action:  0  value:  -0.13170731707317074\n",
      "state:  (13, 4)  action:  0  value:  -0.19469026548672566\n",
      "state:  (13, 5)  action:  0  value:  -0.2653061224489796\n",
      "state:  (13, 6)  action:  0  value:  -0.17703349282296652\n",
      "state:  (13, 7)  action:  0  value:  -0.3064516129032258\n",
      "state:  (13, 8)  action:  0  value:  -0.5495495495495496\n",
      "state:  (13, 9)  action:  0  value:  -0.5132743362831859\n",
      "state:  (13, 10)  action:  0  value:  -0.578838174273859\n",
      "state:  (14, 1)  action:  0  value:  -0.7280701754385965\n",
      "state:  (14, 2)  action:  0  value:  -0.2742616033755274\n",
      "state:  (14, 3)  action:  0  value:  -0.1717171717171717\n",
      "state:  (14, 4)  action:  0  value:  -0.18627450980392157\n",
      "state:  (14, 5)  action:  0  value:  -0.11790393013100436\n",
      "state:  (14, 6)  action:  0  value:  -0.1013215859030837\n",
      "state:  (14, 7)  action:  1  value:  -0.48514351982177706\n",
      "state:  (14, 8)  action:  0  value:  -0.5428571428571428\n",
      "state:  (14, 9)  action:  0  value:  -0.43478260869565216\n",
      "state:  (14, 10)  action:  0  value:  -0.6125290023201856\n",
      "state:  (15, 1)  action:  1  value:  -0.7666017796483044\n",
      "state:  (15, 2)  action:  0  value:  -0.17012448132780084\n",
      "state:  (15, 3)  action:  0  value:  -0.21973094170403587\n",
      "state:  (15, 4)  action:  0  value:  -0.1\n",
      "state:  (15, 5)  action:  0  value:  -0.1896551724137931\n",
      "state:  (15, 6)  action:  0  value:  -0.10309278350515463\n",
      "state:  (15, 7)  action:  1  value:  -0.560328492748792\n",
      "state:  (15, 8)  action:  0  value:  -0.45918367346938777\n",
      "state:  (15, 9)  action:  1  value:  -0.6026168551214997\n",
      "state:  (15, 10)  action:  0  value:  -0.5985832349468713\n",
      "state:  (16, 1)  action:  0  value:  -0.75\n",
      "state:  (16, 2)  action:  0  value:  -0.42105263157894735\n",
      "state:  (16, 3)  action:  0  value:  -0.2571428571428571\n",
      "state:  (16, 4)  action:  0  value:  -0.23478260869565218\n",
      "state:  (16, 5)  action:  0  value:  -0.1324200913242009\n",
      "state:  (16, 6)  action:  0  value:  -0.16908212560386474\n",
      "state:  (16, 7)  action:  0  value:  -0.5648148148148148\n",
      "state:  (16, 8)  action:  0  value:  -0.5213675213675214\n",
      "state:  (16, 9)  action:  0  value:  -0.5844155844155844\n",
      "state:  (16, 10)  action:  0  value:  -0.5767918088737202\n",
      "state:  (17, 1)  action:  0  value:  -0.639269406392694\n",
      "state:  (17, 2)  action:  0  value:  -0.12017167381974249\n",
      "state:  (17, 3)  action:  0  value:  -0.08444444444444445\n",
      "state:  (17, 4)  action:  0  value:  -0.10362694300518134\n",
      "state:  (17, 5)  action:  0  value:  -0.0995260663507109\n",
      "state:  (17, 6)  action:  0  value:  0.10434782608695652\n",
      "state:  (17, 7)  action:  0  value:  -0.03804347826086957\n",
      "state:  (17, 8)  action:  0  value:  -0.37264150943396224\n",
      "state:  (17, 9)  action:  0  value:  -0.41237113402061853\n",
      "state:  (17, 10)  action:  0  value:  -0.4632438739789965\n",
      "state:  (18, 1)  action:  0  value:  -0.3632075471698113\n",
      "state:  (18, 2)  action:  0  value:  0.23115577889447236\n",
      "state:  (18, 3)  action:  0  value:  0.08530805687203792\n",
      "state:  (18, 4)  action:  0  value:  0.12195121951219512\n",
      "state:  (18, 5)  action:  0  value:  0.3333333333333333\n",
      "state:  (18, 6)  action:  0  value:  0.3440366972477064\n",
      "state:  (18, 7)  action:  0  value:  0.45739910313901344\n",
      "state:  (18, 8)  action:  0  value:  0.03418803418803419\n",
      "state:  (18, 9)  action:  0  value:  -0.13471502590673576\n",
      "state:  (18, 10)  action:  0  value:  -0.3105446118192352\n",
      "state:  (19, 1)  action:  0  value:  -0.0625\n",
      "state:  (19, 2)  action:  0  value:  0.3177570093457944\n",
      "state:  (19, 3)  action:  0  value:  0.3791469194312796\n",
      "state:  (19, 4)  action:  0  value:  0.43157894736842106\n",
      "state:  (19, 5)  action:  0  value:  0.42790697674418604\n",
      "state:  (19, 6)  action:  0  value:  0.5679611650485437\n",
      "state:  (19, 7)  action:  0  value:  0.6609442060085837\n",
      "state:  (19, 8)  action:  0  value:  0.5972222222222222\n",
      "state:  (19, 9)  action:  0  value:  0.2184873949579832\n",
      "state:  (19, 10)  action:  0  value:  -0.03970223325062035\n",
      "state:  (20, 1)  action:  0  value:  0.17253521126760563\n",
      "state:  (20, 2)  action:  0  value:  0.6190476190476191\n",
      "state:  (20, 3)  action:  0  value:  0.7044673539518901\n",
      "state:  (20, 4)  action:  0  value:  0.6594427244582043\n",
      "state:  (20, 5)  action:  0  value:  0.6241379310344828\n",
      "state:  (20, 6)  action:  0  value:  0.7198581560283688\n",
      "state:  (20, 7)  action:  0  value:  0.796875\n",
      "state:  (20, 8)  action:  0  value:  0.8369230769230769\n",
      "state:  (20, 9)  action:  0  value:  0.75625\n",
      "state:  (20, 10)  action:  0  value:  0.4103194103194103\n",
      "state:  (21, 1)  action:  0  value:  0.6305418719211823\n",
      "state:  (21, 2)  action:  0  value:  0.9521276595744681\n",
      "state:  (21, 3)  action:  0  value:  0.9306930693069307\n",
      "state:  (21, 4)  action:  0  value:  0.945273631840796\n",
      "state:  (21, 5)  action:  0  value:  0.9481132075471698\n",
      "state:  (21, 6)  action:  0  value:  0.9471153846153846\n",
      "state:  (21, 7)  action:  0  value:  0.9751243781094527\n",
      "state:  (21, 8)  action:  0  value:  0.9801980198019802\n",
      "state:  (21, 9)  action:  0  value:  0.9553072625698324\n",
      "state:  (21, 10)  action:  0  value:  0.9143576826196473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    env = gym.make('Blackjack-v1')\n",
    "    space_size = env.action_space.n\n",
    "    possibleActions = []\n",
    "    for i in range(space_size):\n",
    "        possibleActions.append(str(i))\n",
    "        \n",
    "    # Initialize a Monte-Carlo Agent\n",
    "    agent = MonteCarloAgent(possibleActions,discountFactor = 0.999, epsilon = 0.99)\n",
    "\n",
    "    j = 0\n",
    "    cumulativeReward = 0\n",
    "    # Run training Monte Carlo Method\n",
    "    for episode in range(50000):\n",
    "        j += 1\n",
    "        agent.reset()\n",
    "        observation = env.reset()   # Returns current state\n",
    "\n",
    "        done = False\n",
    "    \n",
    "        while done==False:\n",
    "            \n",
    "            epsilon = agent.computeHyperparameters()\n",
    "            agent.setEpsilon(epsilon)\n",
    "            obsCopy = observation   # Copy current state\n",
    "            agent.setState(agent.toStateRepresentation(obsCopy))\n",
    "            action = agent.act()\n",
    "\n",
    "            nextObservation, reward, done, status = env.step(int(action))\n",
    "            \n",
    "            cumulativeReward+= reward\n",
    "            \n",
    "            \n",
    "            agent.setExperience(agent.toStateRepresentation(obsCopy), str(action), reward)\n",
    "            observation = nextObservation\n",
    "            \n",
    "\n",
    "        QValues = agent.learn()\n",
    "        \n",
    "       \n",
    "\n",
    "    print(\"Q value function: \")\n",
    "    m = 0\n",
    "    for k, v in sorted(QValues.items()):\n",
    "        m +=1\n",
    "        print(\"item \",m,k,v)\n",
    "        \n",
    "    for k, v in sorted(QValues.items()):\n",
    "        print(\"state: \",k,\" action: \",max(v.items(), key=operator.itemgetter(1))[0], \" value: \",max(v.items(), key=operator.itemgetter(1))[1]) \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
